# Text-Summarization

[![Python 3.7](https://img.shields.io/badge/python-3.7-blue.svg)](https://www.python.org/downloads/release/python-380/)
[![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)](https://github.com/jonahwinninghoff/Springboard/graphs/commit-activity)
[![License Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)

**[Overview](#overview)** | **[Findings](#findings)** | **[Transformers](#transformers)** | **[Big Bird](#bigbird)** | **[Method](#method)** | **[Dataset](#dataset)** | **[Insights](#insights)** | **[Future](#future)** | **[Conclusion](#concluding)** | **[References](#refer)**

## OVERVIEW <a id='overview'></a>

<p align = 'justify'> For this research, two text summarizations are compared using a specific metrics and a timer. Two text summarizations outlined in this research are the Big Bird and XLNET. The set of metrics applied to the comparison is Recall-Oriented Understudy for Gisting Evaluation (ROUGE). The timer with CPU 1.6 GHZ is included to assess the algorithmic efficiency. Both algorithms are transferred learnings. Transformers are known for solving various Natural Language Processing (NLP) tasks such as text generation and chatbot. What leads to this question is a fundamental question to ask. The Google Research team attempts to develop a different approach to address the inherent self-attention mechanism problem in Transformers models called the block sparsity. This research team uses a mathematical assessment to demonstrate the block sparsity that helps reduce this quadratic dependency to linear (in relationship of the number of tokens and memory or time) (Zaheer et al., ), which is skeptical.</p>

## KEY FINDINGS <a id = 'findings'></a>

<ul>
  <li><p align = 'justify'> As indicated by Randomized Controlled Trial analysis, the Big Bird model performance is significantly higher than XLNET at Bonferroni correction level. </p></li>
  <li><p align = 'justify'>However, XLNET outperforms Big Bird model in memory efficiency based on producing text prediction per article text.</p></li>
  <li><p align = 'justify'>There is evidence that the model produces some redundancies produced by Big Bird text summarization.</p></li>
  <li><p align = 'justify'>On another hand, the evidence shows that the Big Bird model reduces quadratic dependency to linear against my hypothesis.</p></li>
</ul>

## TRANSFORMERS <a id='transformers'></a>

<img src='https://github.com/jonahwinninghoff/Text-Summarization/blob/main/Images/Transformers%20Architecture.png?raw=true'/>

## BIG BIRD <a id ='bigbird'></a>

""

## METHOD <a id ='method'></a> 
""

## DATASET <a id ='dataset'></a>
""

## ACTIONABLE INSIGHTS <a id ='insights'></a>

""

## FUTURE RESEARCH <a id = 'future'></a>

""

## CONCLUSION <a id='concluding'></a>


## REFERENCES <a id = 'refer'></a>

""

